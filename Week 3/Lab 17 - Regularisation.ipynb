{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff45eed0-ab86-4a0c-9f5c-5083184c8ab6",
   "metadata": {},
   "source": [
    "# Regularised cost and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60fc3ea-e81f-4273-aa04-90fe2b71d4b5",
   "metadata": {},
   "source": [
    "In this lab, we will focus on: \n",
    "* extending the linear and logistic cost functions with a regularisation term. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652f1e11-f942-49b2-92f8-113c36166db4",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0332f765-44d5-4eb0-889f-577f3c445c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_overfit import overfit_example, output\n",
    "from lab_utils_common import sigmoid \n",
    "np.set_printoptions(precision = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb430f-19a2-4393-b26a-ecdecd0bf500",
   "metadata": {},
   "source": [
    "## Adding regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f846169-8e00-4e63-a848-871934ff4342",
   "metadata": {},
   "source": [
    "Remember, that adding a regularisation term does not change the general formula of updating w, and b. Only f(w,b) differs between the linear and logistic regression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3aff73-f9f9-41ff-a85e-31944ea945a0",
   "metadata": {},
   "source": [
    "## Cost function for regularised linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61648046-aaed-4bf7-8566-d086ece3172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args: \n",
    "        X (ndarray (m,n)): Data, m examples with n features\n",
    "        y (ndarray (m,n)): target values\n",
    "        w (ndarray (n))  : model parameters\n",
    "        b (scalar)       : model parameter\n",
    "        lambda_ (scalar) : controls amount of regularisation\n",
    "    Returns: \n",
    "        total_cost (scalar) : cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0] # training set size\n",
    "    n = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m): #loop over each data sample\n",
    "        f_wb_i = np.dot(X[i], w) + b      # calculate the predicted value (scalar)\n",
    "        cost = cost + (f_wb_i - y[i])**2  # calculate the difference (scalar)\n",
    "    cost = cost / (2*m)                   # average cost (scalar)\n",
    "\n",
    "    # Add the regularisation term\n",
    "    reg_cost = 0\n",
    "    for j in range(n): #loop over each feature\n",
    "        reg_cost += (w[j]**2)\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost \n",
    "\n",
    "    total_cost = cost + reg_cost          # scalar\n",
    "    return total_cost                     # scalar \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5780d4d-c438-49fd-8b99-3a595d240497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.0, Regularized cost: 0.047730959406754245\n",
      "Lambda: 0.25, Regularized cost: 0.05896004290510729\n",
      "Lambda: 0.5, Regularized cost: 0.07018912640346034\n",
      "Lambda: 0.75, Regularized cost: 0.08141820990181338\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,) - 0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = np.arange(0,1,0.25)\n",
    "for i in range(0,len(lambda_tmp)):\n",
    "    cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp[i])\n",
    "    print(f\"Lambda: {lambda_tmp[i]}, Regularized cost: {cost_tmp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b704a-659c-4bbe-ba02-234b63b43512",
   "metadata": {},
   "source": [
    "## Cost function for regularised logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea28a997-be88-4b8e-8f7a-28929cac71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same as before, except this time we add a sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa3acd7-7e0c-437e-a69a-91e7e07fe037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "    # Compute normal cost\n",
    "    m,n = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m): #loops over all training data\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost += -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "    cost = cost / m\n",
    "\n",
    "    # Compute regularised term\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost\n",
    "\n",
    "    # Combine\n",
    "    total_cost = cost + reg_cost \n",
    "    return total_cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a603b6ea-f83b-4f58-be65-622c60dbe1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.0, Regularized cost: 0.6536434800787787\n",
      "Lambda: 0.25, Regularized cost: 0.6648725635771318\n",
      "Lambda: 0.5, Regularized cost: 0.6761016470754848\n",
      "Lambda: 0.75, Regularized cost: 0.6873307305738379\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,) - 0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = np.arange(0,1,0.25)\n",
    "for i in range(0,len(lambda_tmp)):\n",
    "    cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp[i])\n",
    "    print(f\"Lambda: {lambda_tmp[i]}, Regularized cost: {cost_tmp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2f57e-b00b-4b4b-921b-66d4a15ba782",
   "metadata": {},
   "source": [
    "# Gradient Descent with regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcecdc4-f6ef-426a-8eb1-25688d9533b9",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a068254-3d3d-49cf-9853-93a019dffd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "    \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8a1a40-01a7-4f4f-9f24-02168c4d0f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.6648774569425726\n",
      "Regularized dj_dw:\n",
      " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e72bb4-a18a-40ea-a3c1-e066a0dd250e",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e42e59-377b-487a-bb9d-5581dc85cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33896983-9f99-468f-b7df-3e9f18380f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.341798994972791\n",
      "Regularized dj_dw:\n",
      " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a8f26-f814-4a7c-8f85-70a5effbbaf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
